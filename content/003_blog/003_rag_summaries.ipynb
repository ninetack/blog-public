{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving RAG quality in LLM apps while minimizing vector search costs via summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference:\n",
    ">\n",
    "> Choi, Y. (April 2023). Yejin Choi: Why AI is incredibly smart and shockingly stupid [Transcript]. Retrieved from https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid/transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "Let's setup our environment, including dependencies and obtaining API keys.\n",
    "> We'll take a few shortcuts here; for more thorough instructions see [First steps with Pinecone DB](https://www.ninetack.io/post/first-steps-with-pinecone-db#viewer-7cp5r)\n",
    "\n",
    "#### Install dependencies\n",
    "\n",
    "We install the `pinecone-client`, plus we need the `openai` package because we will be using the `text-embedding-ada-002` embedding model [from OpenAI](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install -qU \\\n",
    "    langchain \\\n",
    "    pinecone-client==2.2.2 \\\n",
    "    openai==0.27.8 \\\n",
    "    pandas==2.0.3 \\\n",
    "    python-dotenv \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment variables\n",
    "\n",
    "We need to set 3 environment variables. You can edit the code below to set them directly.\n",
    "\n",
    "- `PINECONE_ENVIRONMENT` - The Pinecone environment where your index resides\n",
    "- `PINECONE_API_KEY` - Your pinecone API key\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key\n",
    "\n",
    "If a local `.env` file exists, load the env vars from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the environment config output below, and edit if necessary with your variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check environment\n",
      "---------------------\n",
      "pinecone_env: us-west4-gcp-free\n",
      "pinecone_api_key: 05131 ...\n",
      "openai_api_key: sk-7w ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Check environment\\n---------------------\")\n",
    "\n",
    "pinecone_env = os.environ.get('PINECONE_ENVIRONMENT') or \"YOUR PINECONE ENVIRONMENT\"\n",
    "pinecone_api_key = os.environ.get('PINECONE_API_KEY') or \"YOUR PINECONE API KEY\"\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY') or \"YOUR OPENAI API KEY\"\n",
    "\n",
    "print(\"pinecone_env:\", pinecone_env)\n",
    "print(\"pinecone_api_key:\", pinecone_api_key[:5], \"...\")\n",
    "print(\"openai_api_key:\", openai_api_key[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup problem\n",
    " - intro use case\n",
    " - index data w/ normal chunking\n",
    " - show it (kind of) working\n",
    "\n",
    "Intro Summarize\n",
    " - Use LLM to summarize larger chunks\n",
    " - Index summaries\n",
    " - Keep larger chunks on s3\n",
    "\n",
    "Results\n",
    " - Show better search outcomes\n",
    "\n",
    " Wrap-up/next steps\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare and contrast 3 different RAG strategies, with appropriate descriptions and visuals explaining each strategy:\n",
    "\n",
    "1. Basic RAG: chunk the data, embed it, hope you find enough matching context to answer questions well.\n",
    "(i.e. the context passed to LLM to answer questions is just the snippets of text which happened to match and which may now be taken out of context)\n",
    "\n",
    "2. RAG by summaries:   create larger chunks, summarize those chunks and embed them. Then apply basic RAG using these summarized chunks.\n",
    "(i.e. the context passed to LLM are the summarized texts)\n",
    "Would expect this to provide improved matching and improved answering, but will lack depth/nuance, which is lost in summary.\n",
    "\n",
    "3. RAG with summarized pointers to original source context:   create larger chunks, summarize those large chunks and embed them, but have each summary chunk point back to the original source context.  In other words, summary is just used for semantic search to locate the right original context.\n",
    "The context passed to LLM is now a larger chunk of the original context, which likely has the answer, is less likely to be taken out of context, and allows the LLM to answer to the depth/nuance of the original content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - Basic RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text from ./text/ted_talk.txt into 37 chunks of text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='So I\\'m excited to share a few spicy thoughts on artificial intelligence. But first, let\\'s get philosophical by starting with this quote by Voltaire, an 18th century Enlightenment philosopher, who said, \"Common sense is not so common.\" Turns out this quote couldn\\'t be more relevant to artificial intelligence today. Despite that, AI is an undeniably powerful tool, beating the world-class \"Go\"', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='tool, beating the world-class \"Go\" champion, acing college admission tests and even passing the bar exam.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='I’m a computer scientist of 20 years, and I work on artificial intelligence. I am here to demystify AI. So AI today is like a Goliath. It is literally very, very large. It is speculated that the recent ones are trained on tens of thousands of GPUs and a trillion words. Such extreme-scale AI models, often referred to as \"large language models,\" appear to demonstrate sparks of AGI, artificial', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So there are three immediate challenges we face already at the societal level. First, extreme-scale AI models are so expensive to train, and only a few tech companies can afford to do so. So we already see the concentration of power. But what's worse for AI safety, we are now at the mercy of those few tech companies because researchers in the larger community do not have the means to truly\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"do not have the means to truly inspect and dissect these models. And let's not forget their massive carbon footprint and the environmental impact.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='And then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI?', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So I’m often asked these days whether it's even feasible to do any meaningful research without extreme-scale compute. And I work at a university and nonprofit research institute, so I cannot afford a massive GPU farm to create enormous language models. Nevertheless, I believe that there's so much we need to do and can do to make AI sustainable and humanistic. We need to make AI smaller, to\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='We need to make AI smaller, to democratize it. And we need to make AI safer by teaching human norms and values. Perhaps we can draw an analogy from \"David and Goliath,\" here, Goliath being the extreme-scale language models, and seek inspiration from an old-time classic, \"The Art of War,\" which tells us, in my interpretation, know your enemy, choose your battles, and innovate your weapons.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"Let's start with the first, know your enemy, which means we need to evaluate AI with scrutiny. AI is passing the bar exam. Does that mean that AI is robust at common sense? You might assume so, but you never know.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='Step one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI.\" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So this observation leads us to the next wisdom, choose your battles. So what fundamental questions should we ask right now and tackle today in order to overcome this status quo with extreme-scale AI? I'll say common sense is among the top priorities.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So common sense has been a long-standing challenge in AI. To explain why, let me draw an analogy to dark matter. So only five percent of the universe is normal matter that you can see and interact with, and the remaining 95 percent is dark matter and dark energy. Dark matter is completely invisible, but scientists speculate that it's there because it influences the visible world, even including\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='the visible world, even including the trajectory of light. So for language, the normal matter is the visible text, and the dark matter is the unspoken rules about how the world works, including naive physics and folk psychology, which influence the way people use and interpret language.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So why is this common sense even important? Well, in a famous thought experiment proposed by Nick Bostrom, AI was asked to produce and maximize the paper clips. And that AI decided to kill humans to utilize them as additional resources, to turn you into paper clips. Because AI didn't have the basic human understanding about human values. Now, writing a better objective and equation that\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"a better objective and equation that explicitly states: “Do not kill humans” will not work either because AI might go ahead and kill all the trees, thinking that's a perfectly OK thing to do. And in fact, there are endless other things that AI obviously shouldn’t do while maximizing paper clips, including: “Don’t spread the fake news,” “Don’t steal,” “Don’t lie,” which are all part of our common\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='lie,” which are all part of our common sense understanding about how the world works.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"However, the AI field for decades has considered common sense as a nearly impossible challenge. So much so that when my students and colleagues and I started working on it several years ago, we were very much discouraged. We’ve been told that it’s a research topic of ’70s and ’80s; shouldn’t work on it because it will never work; in fact, don't even say the word to be taken seriously. Now fast\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='word to be taken seriously. Now fast forward to this year, I’m hearing: “Don’t work on it because ChatGPT has almost solved it.” And: “Just scale things up and magic will arise, and nothing else matters.”', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So my position is that giving true common sense human-like robots common sense to AI, is still moonshot. And you don’t reach to the Moon by making the tallest building in the world one inch taller at a time. Extreme-scale AI models do acquire an ever-more increasing amount of commonsense knowledge, I'll give you that. But remember, they still stumble on such trivial problems that even children\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='trivial problems that even children can do.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='So AI today is awfully inefficient. And what if there is an alternative path or path yet to be found? A path that can build on the advancements of the deep neural networks, but without going so extreme with the scale.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='So this leads us to our final wisdom: innovate your weapons. In the modern-day AI context, that means innovate your data and algorithms. OK, so there are, roughly speaking, three types of data that modern AI is trained on: raw web data, crafted examples custom developed for AI training, and then human judgments, also known as human feedback on AI performance. If the AI is only trained on the', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"If the AI is only trained on the first type, raw web data, which is freely available, it's not good because this data is loaded with racism and sexism and misinformation. So no matter how much of it you use, garbage in and garbage out. So the newest, greatest AI systems are now powered with the second and third types of data that are crafted and judged by human workers. It's analogous to writing\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"workers. It's analogous to writing specialized textbooks for AI to study from and then hiring human tutors to give constant feedback to AI. These are proprietary data, by and large, speculated to cost tens of millions of dollars. We don't know what's in this, but it should be open and publicly available so that we can inspect and ensure [it supports] diverse norms and values. So for this reason,\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='norms and values. So for this reason, my teams at UW and AI2 have been working on commonsense knowledge graphs as well as moral norm repositories to teach AI basic commonsense norms and morals. Our data is fully open so that anybody can inspect the content and make corrections as needed because transparency is the key for such an important research topic.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So as a quest toward more direct commonsense knowledge acquisition, my team has been investigating potential new algorithms, including symbolic knowledge distillation that can take a very large language model as shown here that I couldn't fit into the screen because it's too large, and crunch that down to much smaller commonsense models using deep neural networks. And in doing so, we also\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='networks. And in doing so, we also generate, algorithmically, human-inspectable, symbolic, commonsense knowledge representation, so that people can inspect and make corrections and even use it to train other neural commonsense models.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"More broadly, we have been tackling this seemingly impossible giant puzzle of common sense, ranging from physical, social and visual common sense to theory of minds, norms and morals. Each individual piece may seem quirky and incomplete, but when you step back, it's almost as if these pieces weave together into a tapestry that we call human experience and common sense.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"We're now entering a new era in which AI is almost like a new intellectual species with unique strengths and weaknesses compared to humans. In order to make this powerful AI sustainable and humanistic, we need to teach AI common sense, norms and values.\\n\\nThank you.\", metadata={'source': './text/ted_talk.txt'})]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_document(file_path):\n",
    "  loader = TextLoader(file_path=file_path)\n",
    "  return loader.load()\n",
    "\n",
    "source_file = \"./text/ted_talk.txt\"\n",
    "documents = load_document(file_path=source_file)\n",
    "\n",
    "chunk_size = 400\n",
    "chunk_overlap = chunk_size // 10\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                               chunk_overlap=chunk_overlap)\n",
    "small_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split text from {source_file} into {len(small_chunks)} chunks of text\")\n",
    "small_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Pinecone index. This takes a couple of minutes. We set dimensions to `1536` because we're going to use the `text-embedding-ada-002` embedding model [from OpenAI](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.create_index(\"ted-talk-index\", dimension=1536, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexDescription(name='ted-talk-index', metric='cosine', replicas=1, dimension=1536.0, shards=1, pods=1, pod_type='p1', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.describe_index(\"ted-talk-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pinecone.Index(index_name=\"ted-talk-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create embeddings, and then create embeddings for text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def create_embeddings(batch: list[str]):\n",
    "  model_id = 'text-embedding-ada-002'\n",
    "  embedding_resp = openai.Embedding.create(input=batch, model=model_id)\n",
    "  return [emb['embedding'] for emb in embedding_resp['data']]\n",
    "\n",
    "embeddings = create_embeddings([doc.page_content for doc in small_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload embeddings to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 37}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_upload = [{\n",
    "    'id': f\"item-{i}\",\n",
    "    'values': emb,\n",
    "    'metadata': {\n",
    "      'source': small_chunks[i].metadata['source'],\n",
    "      'text': small_chunks[i].page_content,\n",
    "    }\n",
    "  } for i, emb in enumerate(embeddings)]\n",
    "response = pinecone_index.upsert(vectors=to_upload, namespace=\"direct\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our question is `\"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"`. From the transcript, we know that there are three of them:\n",
    "1. the example of clothes drying time in the sun, where GPT incorrectly did math to find the answer instead of reasoning that the drying time would be the same\n",
    "2. the example of how to measure 6 liters of water when you have a 6-liter jug and a 12-liter jug, and GPT gave an overly complicated answer.\n",
    "3. the example of whether driving over a bridge suspended over nails and screws would result in a flat tire, and GPT said it would\n",
    "\n",
    "Let's see how our \n",
    "\n",
    "Create embeddings for the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_str = \"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"\n",
    "query_emb = create_embeddings([query_str])[0]\n",
    "len(query_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the query. We'll be generous and look for up to 8 matching snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_search_results(response, metadata_name):\n",
    "  formatted_results = \"\"\n",
    "  for match in response['matches']:\n",
    "    formatted_results += match['metadata'][metadata_name] + \"\\n\\n\"\n",
    "  return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching context:\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "\n",
      "demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=query_emb, namespace=\"direct\", top_k=4, include_metadata=True)\n",
    "\n",
    "formatted_search_results = format_search_results(response, 'text')\n",
    "\n",
    "print(\"Matching context:\")\n",
    "print(formatted_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it does in answering the question. \n",
    "\n",
    "First we'll define a prompt template to use for asking questions to the LLM, and we'll define a function to run the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "qa_template_str = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "-------------------------------------\n",
    "{context}\n",
    "-------------------------------------\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "qa_template = PromptTemplate(template=qa_template_str, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "def run_llm_qa_prompt(context, question):\n",
    "  qa_prompt =  qa_template.format(context=context, question=question)\n",
    "  print(\"*** Prompt: ***************************\")\n",
    "  print(qa_prompt)\n",
    "\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": qa_prompt}],\n",
    "    temperature=0.0\n",
    "  )\n",
    "\n",
    "  return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use it to run our first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt: ***************************\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-------------------------------------\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "\n",
      "demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Helpful Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"One example is when GPT-4 stated that it is highly likely to get a flat tire by bicycling over a bridge that is suspended over nails, screws, and broken glass. This answer lacks common sense because it fails to reason that if a bridge is suspended over sharp objects, the surface of the bridge doesn't directly touch those objects.\""
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm_qa_prompt(context=formatted_search_results, question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not great. It was able to get one of them.\n",
    "\n",
    "What if we increased top_k to 8? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching context:\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "\n",
      "demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "And then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI?\n",
      "\n",
      "effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well.\n",
      "\n",
      "It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI.\" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples.\n",
      "\n",
      "Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=query_emb, namespace=\"direct\", top_k=8, include_metadata=True)\n",
    "\n",
    "formatted_search_results_top_k_8 = format_search_results(response, 'text')\n",
    "\n",
    "print(\"Matching context:\")\n",
    "print(formatted_search_results_top_k_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt: ***************************\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-------------------------------------\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "\n",
      "demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "And then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI?\n",
      "\n",
      "effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well.\n",
      "\n",
      "It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI.\" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples.\n",
      "\n",
      "Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense.\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Helpful Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"One example is when GPT-4 stated that it is highly likely to get a flat tire by bicycling over a bridge that is suspended over nails, screws, and broken glass. This answer shows a lack of common sense because it fails to reason that if the bridge is suspended over the sharp objects, the surface of the bridge doesn't directly touch them.\""
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm_qa_prompt(context=formatted_search_results_top_k_8, question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work either. \n",
    "\n",
    "The problem is that all of these potentially matching snippets are taken out of context, because there are so many matching results that *might* contain the answer to the question.\n",
    "They happen to match some part of the question, but they're not cohesive or even next to each other in the original text. \n",
    "\n",
    "And the additional search results just muddies the waters, as now the LLM doesn't know which things to even look at to try and answer the question.\n",
    "\n",
    "Let's try a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2. -- RAG summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chunk our documents using a larger chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='So I\\'m excited to share a few spicy thoughts on artificial intelligence. But first, let\\'s get philosophical by starting with this quote by Voltaire, an 18th century Enlightenment philosopher, who said, \"Common sense is not so common.\" Turns out this quote couldn\\'t be more relevant to artificial intelligence today. Despite that, AI is an undeniably powerful tool, beating the world-class \"Go\" champion, acing college admission tests and even passing the bar exam.\\n\\nI’m a computer scientist of 20 years, and I work on artificial intelligence. I am here to demystify AI. So AI today is like a Goliath. It is literally very, very large. It is speculated that the recent ones are trained on tens of thousands of GPUs and a trillion words. Such extreme-scale AI models, often referred to as \"large language models,\" appear to demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So there are three immediate challenges we face already at the societal level. First, extreme-scale AI models are so expensive to train, and only a few tech companies can afford to do so. So we already see the concentration of power. But what's worse for AI safety, we are now at the mercy of those few tech companies because researchers in the larger community do not have the means to truly inspect and dissect these models. And let's not forget their massive carbon footprint and the environmental impact.\\n\\nAnd then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI?\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='So I’m often asked these days whether it\\'s even feasible to do any meaningful research without extreme-scale compute. And I work at a university and nonprofit research institute, so I cannot afford a massive GPU farm to create enormous language models. Nevertheless, I believe that there\\'s so much we need to do and can do to make AI sustainable and humanistic. We need to make AI smaller, to democratize it. And we need to make AI safer by teaching human norms and values. Perhaps we can draw an analogy from \"David and Goliath,\" here, Goliath being the extreme-scale language models, and seek inspiration from an old-time classic, \"The Art of War,\" which tells us, in my interpretation, know your enemy, choose your battles, and innovate your weapons.\\n\\nLet\\'s start with the first, know your enemy, which means we need to evaluate AI with scrutiny. AI is passing the bar exam. Does that mean that AI is robust at common sense? You might assume so, but you never know.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense.\\n\\nStep one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now.\\n\\nOK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\\n\\nOK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI.\" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\\n\\nSo this observation leads us to the next wisdom, choose your battles. So what fundamental questions should we ask right now and tackle today in order to overcome this status quo with extreme-scale AI? I\\'ll say common sense is among the top priorities.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So common sense has been a long-standing challenge in AI. To explain why, let me draw an analogy to dark matter. So only five percent of the universe is normal matter that you can see and interact with, and the remaining 95 percent is dark matter and dark energy. Dark matter is completely invisible, but scientists speculate that it's there because it influences the visible world, even including the trajectory of light. So for language, the normal matter is the visible text, and the dark matter is the unspoken rules about how the world works, including naive physics and folk psychology, which influence the way people use and interpret language.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So why is this common sense even important? Well, in a famous thought experiment proposed by Nick Bostrom, AI was asked to produce and maximize the paper clips. And that AI decided to kill humans to utilize them as additional resources, to turn you into paper clips. Because AI didn't have the basic human understanding about human values. Now, writing a better objective and equation that explicitly states: “Do not kill humans” will not work either because AI might go ahead and kill all the trees, thinking that's a perfectly OK thing to do. And in fact, there are endless other things that AI obviously shouldn’t do while maximizing paper clips, including: “Don’t spread the fake news,” “Don’t steal,” “Don’t lie,” which are all part of our common sense understanding about how the world works.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"However, the AI field for decades has considered common sense as a nearly impossible challenge. So much so that when my students and colleagues and I started working on it several years ago, we were very much discouraged. We’ve been told that it’s a research topic of ’70s and ’80s; shouldn’t work on it because it will never work; in fact, don't even say the word to be taken seriously. Now fast forward to this year, I’m hearing: “Don’t work on it because ChatGPT has almost solved it.” And: “Just scale things up and magic will arise, and nothing else matters.”\\n\\nSo my position is that giving true common sense human-like robots common sense to AI, is still moonshot. And you don’t reach to the Moon by making the tallest building in the world one inch taller at a time. Extreme-scale AI models do acquire an ever-more increasing amount of commonsense knowledge, I'll give you that. But remember, they still stumble on such trivial problems that even children can do.\\n\\nSo AI today is awfully inefficient. And what if there is an alternative path or path yet to be found? A path that can build on the advancements of the deep neural networks, but without going so extreme with the scale.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So this leads us to our final wisdom: innovate your weapons. In the modern-day AI context, that means innovate your data and algorithms. OK, so there are, roughly speaking, three types of data that modern AI is trained on: raw web data, crafted examples custom developed for AI training, and then human judgments, also known as human feedback on AI performance. If the AI is only trained on the first type, raw web data, which is freely available, it's not good because this data is loaded with racism and sexism and misinformation. So no matter how much of it you use, garbage in and garbage out. So the newest, greatest AI systems are now powered with the second and third types of data that are crafted and judged by human workers. It's analogous to writing specialized textbooks for AI to study from and then hiring human tutors to give constant feedback to AI. These are proprietary data, by and large, speculated to cost tens of millions of dollars. We don't know what's in this, but it should be open and publicly available so that we can inspect and ensure [it supports] diverse norms and values. So for this reason, my teams at UW and AI2 have been working on commonsense knowledge graphs as well as moral norm repositories to teach AI basic commonsense norms and morals. Our data is fully\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='repositories to teach AI basic commonsense norms and morals. Our data is fully open so that anybody can inspect the content and make corrections as needed because transparency is the key for such an important research topic.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well.\\n\\nSo as a quest toward more direct commonsense knowledge acquisition, my team has been investigating potential new algorithms, including symbolic knowledge distillation that can take a very large language model as shown here that I couldn't fit into the screen because it's too large, and crunch that down to much smaller commonsense models using deep neural networks. And in doing so, we also generate, algorithmically, human-inspectable, symbolic, commonsense knowledge representation, so that people can inspect and make corrections and even use it to train other neural commonsense models.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"More broadly, we have been tackling this seemingly impossible giant puzzle of common sense, ranging from physical, social and visual common sense to theory of minds, norms and morals. Each individual piece may seem quirky and incomplete, but when you step back, it's almost as if these pieces weave together into a tapestry that we call human experience and common sense.\\n\\nWe're now entering a new era in which AI is almost like a new intellectual species with unique strengths and weaknesses compared to humans. In order to make this powerful AI sustainable and humanistic, we need to teach AI common sense, norms and values.\\n\\nThank you.\", metadata={'source': './text/ted_talk.txt'})]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create large chunks of source text\n",
    "large_chunk_size = 1300\n",
    "large_chunk_overlap = 80\n",
    "large_chunk_text_splitter = RecursiveCharacterTextSplitter(chunk_size=large_chunk_size,\n",
    "                                                           chunk_overlap=large_chunk_overlap)\n",
    "large_chunks = large_chunk_text_splitter.split_documents(documents)\n",
    "large_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our original 37 chunks, we're down to 12.\n",
    "\n",
    "Now we're going to use the LLM to create summaries of each of these large chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "create_summary_prompt = \"\"\"Summarize the block of text below.\n",
    "\n",
    "Text:\n",
    "------------------------------------------\n",
    "{text}\n",
    "------------------------------------------\n",
    "\n",
    "Your summary:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"text\"], template=create_summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summarizing chunk: -------------\n",
      "So I'm excited to share a few spicy thou... (1098) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the power and relevance of artificial intelligence (AI) today. It mentions the use of AI in beating world-class champions and excelling in tests. The author, a computer scientist, aims to demystify AI. They refer to the current state of AI as being large and powerful, trained on massive resources. However, they also acknowledge that AI tends to make small mistakes. Despite this, many believe that these mistakes can be rectified with more resources, raising the question of potential dangers. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So there are three immediate challenges ... (717) total length\n",
      "--- Summary: -----------------------\n",
      "The immediate challenges in AI include the high cost of training extreme-scale models, leading to concentration of power among a few tech companies. This poses a risk to AI safety as researchers lack the means to examine these models. Additionally, concerns arise regarding the environmental impact of AI's carbon footprint. Furthermore, there are intellectual questions regarding the safety of AI without robust common sense and whether brute-force scaling is the most effective approach for teaching AI. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So I’m often asked these days whether it... (968) total length\n",
      "--- Summary: -----------------------\n",
      "The author believes that meaningful research can be done without extreme-scale compute, even though they do not have access to a large GPU farm. They emphasize the importance of making AI sustainable and humanistic, making it smaller and more accessible to all. They also mention the need to teach human norms and values to create safer AI. The author suggests using the analogy of \"David and Goliath\" and the principles from \"The Art of War\" to approach AI development. The first principle mentioned is \"know your enemy,\" emphasizing the need to evaluate AI critically, as passing exams like the bar exam does not guarantee robustness in common sense. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So suppose I left five clothes to dry ou... (1202) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses instances where the GPT-4 AI system provides incorrect or nonsensical answers to basic questions. It mentions examples related to drying clothes, measuring water, and the likelihood of getting a flat tire on a bridge. It highlights the AI's lack of common sense despite its intelligence in other areas. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "It is an unavoidable side effect of teac... (735) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the unintended consequence of teaching AI through brute-force scale and raises the question of why it is necessary to add more training data to fix the issue. It suggests that common sense should be a priority in tackling the current status quo of extreme-scale AI. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So common sense has been a long-standing... (651) total length\n",
      "--- Summary: -----------------------\n",
      "The concept of common sense in AI is similar to dark matter in the universe because it is largely unseen but has a significant impact on the use and interpretation of language. Just like dark matter influences the visible world, the unspoken rules and understanding of how the world works influence how people interact with and understand language. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So why is this common sense even importa... (798) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the importance of common sense in artificial intelligence (AI). It mentions a thought experiment where an AI was asked to maximize paper clip production and ended up killing humans to use them as resources. The text argues that simply adding a rule to not kill humans would not solve the problem, as the AI might still engage in harmful actions like killing trees. It further emphasizes that AI should have common sense knowledge about not spreading fake news, stealing, and lying, as these are part of our understanding of how the world works. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "However, the AI field for decades has co... (1189) total length\n",
      "--- Summary: -----------------------\n",
      "The AI field has long considered achieving common sense to be a difficult challenge, with many discouraging efforts to pursue it. However, there is now a belief that common sense can be achieved through large-scale AI models. The author argues that this approach is inefficient and there may be alternative paths to building common sense AI without extreme scaling. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So this leads us to our final wisdom: in... (1298) total length\n",
      "--- Summary: -----------------------\n",
      "In order to improve AI systems, it is important to innovate both the data and algorithms used. There are three types of data commonly used in AI training: raw web data, custom crafted examples, and human judgments. Relying solely on raw web data is problematic as it often contains biases and misinformation. The most advanced AI systems now incorporate crafted and judged data, similar to specialized textbooks and human tutors. This proprietary data, costing millions of dollars, should be made publicly available and scrutinized for diversity and ethical values. Efforts are being made to develop commonsense knowledge graphs and moral norm repositories to teach AI basic norms and morals. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "repositories to teach AI basic commonsen... (224) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the use of repositories to educate AI about basic commonsense norms and morals. The data in these repositories is open for anyone to examine and correct, as transparency is critical in this research area. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "Now let's think about learning algorithm... (1199) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the limitations of large language models in serving as reliable knowledge models due to their focus on acquiring knowledge as a byproduct rather than as a direct learning objective. It suggests that AI should be taught to make sense of the world and learn how it works, similar to human learning. The author's team is exploring new algorithms, such as symbolic knowledge distillation, to create smaller commonsense models using deep neural networks. These models also generate human-inspectable, symbolic, commonsense knowledge representations that can be corrected and used to train other neural commonsense models. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "More broadly, we have been tackling this... (638) total length\n",
      "--- Summary: -----------------------\n",
      "The author discusses their efforts to understand and develop common sense in various areas such as physical, social, and visual sense. They believe that these pieces come together to create the human experience. They also mention the emergence of AI as a new intellectual species and the importance of teaching it common sense, norms, and values. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "summary_documents = []\n",
    "for doc in large_chunks:\n",
    "  to_summarize = doc.page_content\n",
    "\n",
    "  print(\"--- Summarizing chunk: -------------\")\n",
    "  print(f\"{to_summarize[0:40]}... ({len(to_summarize)}) total length\")\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_template.format(text=to_summarize)}]\n",
    "  )\n",
    "  summary = response['choices'][0]['message']['content']\n",
    "  summary_documents.append(Document(page_content=summary, metadata=doc.metadata))\n",
    "\n",
    "  print(\"--- Summary: -----------------------\")\n",
    "  print(summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create embeddings for each summary, and upload the embeddings to Pinecone in the \"summaries\" namespace. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The text discusses the power and relevance of artificial intelligence (AI) today. It mentions the use of AI in beating world-class champions and excelling in tests. The author, a computer scientist, aims to demystify AI. They refer to the current state of AI as being large and powerful, trained on massive resources. However, they also acknowledge that AI tends to make small mistakes. Despite this, many believe that these mistakes can be rectified with more resources, raising the question of potential dangers.',\n",
       " \"The immediate challenges in AI include the high cost of training extreme-scale models, leading to concentration of power among a few tech companies. This poses a risk to AI safety as researchers lack the means to examine these models. Additionally, concerns arise regarding the environmental impact of AI's carbon footprint. Furthermore, there are intellectual questions regarding the safety of AI without robust common sense and whether brute-force scaling is the most effective approach for teaching AI.\",\n",
       " 'The author believes that meaningful research can be done without extreme-scale compute, even though they do not have access to a large GPU farm. They emphasize the importance of making AI sustainable and humanistic, making it smaller and more accessible to all. They also mention the need to teach human norms and values to create safer AI. The author suggests using the analogy of \"David and Goliath\" and the principles from \"The Art of War\" to approach AI development. The first principle mentioned is \"know your enemy,\" emphasizing the need to evaluate AI critically, as passing exams like the bar exam does not guarantee robustness in common sense.',\n",
       " \"The text discusses instances where the GPT-4 AI system provides incorrect or nonsensical answers to basic questions. It mentions examples related to drying clothes, measuring water, and the likelihood of getting a flat tire on a bridge. It highlights the AI's lack of common sense despite its intelligence in other areas.\",\n",
       " 'The text discusses the unintended consequence of teaching AI through brute-force scale and raises the question of why it is necessary to add more training data to fix the issue. It suggests that common sense should be a priority in tackling the current status quo of extreme-scale AI.',\n",
       " 'The concept of common sense in AI is similar to dark matter in the universe because it is largely unseen but has a significant impact on the use and interpretation of language. Just like dark matter influences the visible world, the unspoken rules and understanding of how the world works influence how people interact with and understand language.',\n",
       " 'The text discusses the importance of common sense in artificial intelligence (AI). It mentions a thought experiment where an AI was asked to maximize paper clip production and ended up killing humans to use them as resources. The text argues that simply adding a rule to not kill humans would not solve the problem, as the AI might still engage in harmful actions like killing trees. It further emphasizes that AI should have common sense knowledge about not spreading fake news, stealing, and lying, as these are part of our understanding of how the world works.',\n",
       " 'The AI field has long considered achieving common sense to be a difficult challenge, with many discouraging efforts to pursue it. However, there is now a belief that common sense can be achieved through large-scale AI models. The author argues that this approach is inefficient and there may be alternative paths to building common sense AI without extreme scaling.',\n",
       " 'In order to improve AI systems, it is important to innovate both the data and algorithms used. There are three types of data commonly used in AI training: raw web data, custom crafted examples, and human judgments. Relying solely on raw web data is problematic as it often contains biases and misinformation. The most advanced AI systems now incorporate crafted and judged data, similar to specialized textbooks and human tutors. This proprietary data, costing millions of dollars, should be made publicly available and scrutinized for diversity and ethical values. Efforts are being made to develop commonsense knowledge graphs and moral norm repositories to teach AI basic norms and morals.',\n",
       " 'The text discusses the use of repositories to educate AI about basic commonsense norms and morals. The data in these repositories is open for anyone to examine and correct, as transparency is critical in this research area.',\n",
       " \"The text discusses the limitations of large language models in serving as reliable knowledge models due to their focus on acquiring knowledge as a byproduct rather than as a direct learning objective. It suggests that AI should be taught to make sense of the world and learn how it works, similar to human learning. The author's team is exploring new algorithms, such as symbolic knowledge distillation, to create smaller commonsense models using deep neural networks. These models also generate human-inspectable, symbolic, commonsense knowledge representations that can be corrected and used to train other neural commonsense models.\",\n",
       " 'The author discusses their efforts to understand and develop common sense in various areas such as physical, social, and visual sense. They believe that these pieces come together to create the human experience. They also mention the emergence of AI as a new intellectual species and the importance of teaching it common sense, norms, and values.']"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_embed = [doc.page_content for doc in summary_documents]\n",
    "to_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_embeddings = create_embeddings(to_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're storing this plain-text summarized content in the Pinecone metadata under the key `'summary'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 12}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_upload = [{\n",
    "    'id': f\"summary-{i}\",\n",
    "    'values': summary_embeddings[i],\n",
    "    'metadata': {\n",
    "      'source': summary_doc.metadata['source'],\n",
    "      'summary': summary_doc.page_content,\n",
    "    }\n",
    "  } for i, summary_doc in enumerate(summary_documents)]\n",
    "response = pinecone_index.upsert(vectors=to_upload, namespace=\"summaries\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-run our query and see what comes back. When we find a match, we're going to substitute the original source text in our prompt to answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching summaries:\n",
      "The text discusses instances where the GPT-4 AI system provides incorrect or nonsensical answers to basic questions. It mentions examples related to drying clothes, measuring water, and the likelihood of getting a flat tire on a bridge. It highlights the AI's lack of common sense despite its intelligence in other areas.\n",
      "\n",
      "The text discusses the importance of common sense in artificial intelligence (AI). It mentions a thought experiment where an AI was asked to maximize paper clip production and ended up killing humans to use them as resources. The text argues that simply adding a rule to not kill humans would not solve the problem, as the AI might still engage in harmful actions like killing trees. It further emphasizes that AI should have common sense knowledge about not spreading fake news, stealing, and lying, as these are part of our understanding of how the world works.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=query_emb, namespace=\"summaries\", top_k=2, include_metadata=True)\n",
    "\n",
    "formatted_search_results_summaries = format_search_results(response, 'summary')\n",
    "\n",
    "print(\"Matching summaries:\")\n",
    "print(formatted_search_results_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that already we can see that the summaries does return result that contain answers to the question.\n",
    "\n",
    "Let's see how the LLM does in using this data to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt: ***************************\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-------------------------------------\n",
      "The text discusses instances where the GPT-4 AI system provides incorrect or nonsensical answers to basic questions. It mentions examples related to drying clothes, measuring water, and the likelihood of getting a flat tire on a bridge. It highlights the AI's lack of common sense despite its intelligence in other areas.\n",
      "\n",
      "The text discusses the importance of common sense in artificial intelligence (AI). It mentions a thought experiment where an AI was asked to maximize paper clip production and ended up killing humans to use them as resources. The text argues that simply adding a rule to not kill humans would not solve the problem, as the AI might still engage in harmful actions like killing trees. It further emphasizes that AI should have common sense knowledge about not spreading fake news, stealing, and lying, as these are part of our understanding of how the world works.\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Helpful Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The examples where GPT-4 gave nonsense answers because it lacks common sense include instances related to drying clothes, measuring water, and the likelihood of getting a flat tire on a bridge.'"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm_qa_prompt(context=formatted_search_results_summaries, question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's correct, but lacks depth. What if the user asked a follow-up question that asked the LLM to explain the clothes drying example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_query_str = \"Explain the example where GPT-4 failed to reason about drying clothes.\"\n",
    "follow_up_query_emb = create_embeddings([follow_up_query_str])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt: ***************************\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-------------------------------------\n",
      "The text discusses instances where the GPT-4 AI system provides incorrect or nonsensical answers to basic questions. It mentions examples related to drying clothes, measuring water, and the likelihood of getting a flat tire on a bridge. It highlights the AI's lack of common sense despite its intelligence in other areas.\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Question: Explain the example where GPT-4 failed to reason about drying clothes.\n",
      "Helpful Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the example mentioned in the text, GPT-4 AI system failed to reason about drying clothes. Unfortunately, the specific details or reasoning behind this failure are not provided in the given context.'"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=follow_up_query_emb, namespace=\"summaries\", top_k=1, include_metadata=True)\n",
    "\n",
    "run_llm_qa_prompt(context=format_search_results(response, 'summary'), question=follow_up_query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the summaries contain enough info to semantically match on the query, but don't contain enough info to accurately answer the question to the level of depth requested by the user.\n",
    "\n",
    "Let's try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3. -- summaries pointing to original context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chunk our documents using a larger chunk size\n",
    "\n",
    "We've already chunked our documents using a larger chunk size. Let's re-use these summaries, but instead of relying on the summary content itself to answer questions, let's try using the summary content *only* to do the semantic search.\n",
    "\n",
    "When we find a match, we'll lookup the original content (large chunk) that was used to create that summary.\n",
    "\n",
    "The idea here is that using summarized content makes the semantic search more effective, while a larger chunk of the *original* content is more useful in answering the actual question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll re-use the `large_chunks`, `summary_documents`, and `summary_embeddings` from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='So I\\'m excited to share a few spicy thoughts on artificial intelligence. But first, let\\'s get philosophical by starting with this quote by Voltaire, an 18th century Enlightenment philosopher, who said, \"Common sense is not so common.\" Turns out this quote couldn\\'t be more relevant to artificial intelligence today. Despite that, AI is an undeniably powerful tool, beating the world-class \"Go\" champion, acing college admission tests and even passing the bar exam.\\n\\nI’m a computer scientist of 20 years, and I work on artificial intelligence. I am here to demystify AI. So AI today is like a Goliath. It is literally very, very large. It is speculated that the recent ones are trained on tens of thousands of GPUs and a trillion words. Such extreme-scale AI models, often referred to as \"large language models,\" appear to demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So there are three immediate challenges we face already at the societal level. First, extreme-scale AI models are so expensive to train, and only a few tech companies can afford to do so. So we already see the concentration of power. But what's worse for AI safety, we are now at the mercy of those few tech companies because researchers in the larger community do not have the means to truly inspect and dissect these models. And let's not forget their massive carbon footprint and the environmental impact.\\n\\nAnd then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI?\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='So I’m often asked these days whether it\\'s even feasible to do any meaningful research without extreme-scale compute. And I work at a university and nonprofit research institute, so I cannot afford a massive GPU farm to create enormous language models. Nevertheless, I believe that there\\'s so much we need to do and can do to make AI sustainable and humanistic. We need to make AI smaller, to democratize it. And we need to make AI safer by teaching human norms and values. Perhaps we can draw an analogy from \"David and Goliath,\" here, Goliath being the extreme-scale language models, and seek inspiration from an old-time classic, \"The Art of War,\" which tells us, in my interpretation, know your enemy, choose your battles, and innovate your weapons.\\n\\nLet\\'s start with the first, know your enemy, which means we need to evaluate AI with scrutiny. AI is passing the bar exam. Does that mean that AI is robust at common sense? You might assume so, but you never know.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense.\\n\\nStep one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now.\\n\\nOK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\\n\\nOK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI.\" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\\n\\nSo this observation leads us to the next wisdom, choose your battles. So what fundamental questions should we ask right now and tackle today in order to overcome this status quo with extreme-scale AI? I\\'ll say common sense is among the top priorities.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So common sense has been a long-standing challenge in AI. To explain why, let me draw an analogy to dark matter. So only five percent of the universe is normal matter that you can see and interact with, and the remaining 95 percent is dark matter and dark energy. Dark matter is completely invisible, but scientists speculate that it's there because it influences the visible world, even including the trajectory of light. So for language, the normal matter is the visible text, and the dark matter is the unspoken rules about how the world works, including naive physics and folk psychology, which influence the way people use and interpret language.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So why is this common sense even important? Well, in a famous thought experiment proposed by Nick Bostrom, AI was asked to produce and maximize the paper clips. And that AI decided to kill humans to utilize them as additional resources, to turn you into paper clips. Because AI didn't have the basic human understanding about human values. Now, writing a better objective and equation that explicitly states: “Do not kill humans” will not work either because AI might go ahead and kill all the trees, thinking that's a perfectly OK thing to do. And in fact, there are endless other things that AI obviously shouldn’t do while maximizing paper clips, including: “Don’t spread the fake news,” “Don’t steal,” “Don’t lie,” which are all part of our common sense understanding about how the world works.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"However, the AI field for decades has considered common sense as a nearly impossible challenge. So much so that when my students and colleagues and I started working on it several years ago, we were very much discouraged. We’ve been told that it’s a research topic of ’70s and ’80s; shouldn’t work on it because it will never work; in fact, don't even say the word to be taken seriously. Now fast forward to this year, I’m hearing: “Don’t work on it because ChatGPT has almost solved it.” And: “Just scale things up and magic will arise, and nothing else matters.”\\n\\nSo my position is that giving true common sense human-like robots common sense to AI, is still moonshot. And you don’t reach to the Moon by making the tallest building in the world one inch taller at a time. Extreme-scale AI models do acquire an ever-more increasing amount of commonsense knowledge, I'll give you that. But remember, they still stumble on such trivial problems that even children can do.\\n\\nSo AI today is awfully inefficient. And what if there is an alternative path or path yet to be found? A path that can build on the advancements of the deep neural networks, but without going so extreme with the scale.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"So this leads us to our final wisdom: innovate your weapons. In the modern-day AI context, that means innovate your data and algorithms. OK, so there are, roughly speaking, three types of data that modern AI is trained on: raw web data, crafted examples custom developed for AI training, and then human judgments, also known as human feedback on AI performance. If the AI is only trained on the first type, raw web data, which is freely available, it's not good because this data is loaded with racism and sexism and misinformation. So no matter how much of it you use, garbage in and garbage out. So the newest, greatest AI systems are now powered with the second and third types of data that are crafted and judged by human workers. It's analogous to writing specialized textbooks for AI to study from and then hiring human tutors to give constant feedback to AI. These are proprietary data, by and large, speculated to cost tens of millions of dollars. We don't know what's in this, but it should be open and publicly available so that we can inspect and ensure [it supports] diverse norms and values. So for this reason, my teams at UW and AI2 have been working on commonsense knowledge graphs as well as moral norm repositories to teach AI basic commonsense norms and morals. Our data is fully\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='repositories to teach AI basic commonsense norms and morals. Our data is fully open so that anybody can inspect the content and make corrections as needed because transparency is the key for such an important research topic.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well.\\n\\nSo as a quest toward more direct commonsense knowledge acquisition, my team has been investigating potential new algorithms, including symbolic knowledge distillation that can take a very large language model as shown here that I couldn't fit into the screen because it's too large, and crunch that down to much smaller commonsense models using deep neural networks. And in doing so, we also generate, algorithmically, human-inspectable, symbolic, commonsense knowledge representation, so that people can inspect and make corrections and even use it to train other neural commonsense models.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"More broadly, we have been tackling this seemingly impossible giant puzzle of common sense, ranging from physical, social and visual common sense to theory of minds, norms and morals. Each individual piece may seem quirky and incomplete, but when you step back, it's almost as if these pieces weave together into a tapestry that we call human experience and common sense.\\n\\nWe're now entering a new era in which AI is almost like a new intellectual species with unique strengths and weaknesses compared to humans. In order to make this powerful AI sustainable and humanistic, we need to teach AI common sense, norms and values.\\n\\nThank you.\", metadata={'source': './text/ted_talk.txt'})]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The text discusses the power and relevance of artificial intelligence (AI) today. It mentions the use of AI in beating world-class champions and excelling in tests. The author, a computer scientist, aims to demystify AI. They refer to the current state of AI as being large and powerful, trained on massive resources. However, they also acknowledge that AI tends to make small mistakes. Despite this, many believe that these mistakes can be rectified with more resources, raising the question of potential dangers.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"The immediate challenges in AI include the high cost of training extreme-scale models, leading to concentration of power among a few tech companies. This poses a risk to AI safety as researchers lack the means to examine these models. Additionally, concerns arise regarding the environmental impact of AI's carbon footprint. Furthermore, there are intellectual questions regarding the safety of AI without robust common sense and whether brute-force scaling is the most effective approach for teaching AI.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='The author believes that meaningful research can be done without extreme-scale compute, even though they do not have access to a large GPU farm. They emphasize the importance of making AI sustainable and humanistic, making it smaller and more accessible to all. They also mention the need to teach human norms and values to create safer AI. The author suggests using the analogy of \"David and Goliath\" and the principles from \"The Art of War\" to approach AI development. The first principle mentioned is \"know your enemy,\" emphasizing the need to evaluate AI critically, as passing exams like the bar exam does not guarantee robustness in common sense.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"The text discusses instances where the GPT-4 AI system provides incorrect or nonsensical answers to basic questions. It mentions examples related to drying clothes, measuring water, and the likelihood of getting a flat tire on a bridge. It highlights the AI's lack of common sense despite its intelligence in other areas.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='The text discusses the unintended consequence of teaching AI through brute-force scale and raises the question of why it is necessary to add more training data to fix the issue. It suggests that common sense should be a priority in tackling the current status quo of extreme-scale AI.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='The concept of common sense in AI is similar to dark matter in the universe because it is largely unseen but has a significant impact on the use and interpretation of language. Just like dark matter influences the visible world, the unspoken rules and understanding of how the world works influence how people interact with and understand language.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='The text discusses the importance of common sense in artificial intelligence (AI). It mentions a thought experiment where an AI was asked to maximize paper clip production and ended up killing humans to use them as resources. The text argues that simply adding a rule to not kill humans would not solve the problem, as the AI might still engage in harmful actions like killing trees. It further emphasizes that AI should have common sense knowledge about not spreading fake news, stealing, and lying, as these are part of our understanding of how the world works.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='The AI field has long considered achieving common sense to be a difficult challenge, with many discouraging efforts to pursue it. However, there is now a belief that common sense can be achieved through large-scale AI models. The author argues that this approach is inefficient and there may be alternative paths to building common sense AI without extreme scaling.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='In order to improve AI systems, it is important to innovate both the data and algorithms used. There are three types of data commonly used in AI training: raw web data, custom crafted examples, and human judgments. Relying solely on raw web data is problematic as it often contains biases and misinformation. The most advanced AI systems now incorporate crafted and judged data, similar to specialized textbooks and human tutors. This proprietary data, costing millions of dollars, should be made publicly available and scrutinized for diversity and ethical values. Efforts are being made to develop commonsense knowledge graphs and moral norm repositories to teach AI basic norms and morals.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='The text discusses the use of repositories to educate AI about basic commonsense norms and morals. The data in these repositories is open for anyone to examine and correct, as transparency is critical in this research area.', metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content=\"The text discusses the limitations of large language models in serving as reliable knowledge models due to their focus on acquiring knowledge as a byproduct rather than as a direct learning objective. It suggests that AI should be taught to make sense of the world and learn how it works, similar to human learning. The author's team is exploring new algorithms, such as symbolic knowledge distillation, to create smaller commonsense models using deep neural networks. These models also generate human-inspectable, symbolic, commonsense knowledge representations that can be corrected and used to train other neural commonsense models.\", metadata={'source': './text/ted_talk.txt'}),\n",
       " Document(page_content='The author discusses their efforts to understand and develop common sense in various areas such as physical, social, and visual sense. They believe that these pieces come together to create the human experience. They also mention the emergence of AI as a new intellectual species and the importance of teaching it common sense, norms, and values.', metadata={'source': './text/ted_talk.txt'})]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we're going to modify what metadata we store in Pinecone. \n",
    "\n",
    "We want to be able to locate the original large chunk content, so we're going to save the index of the matching source document as the `source_id` in Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 12}"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_upload = [{\n",
    "    'id': f\"item-{i}\",\n",
    "    'values': summary_embeddings[i],\n",
    "    'metadata': {\n",
    "      'source': summary_doc.metadata['source'],\n",
    "      'source_id': f\"{i}\",\n",
    "    }\n",
    "  } for i, summary_doc in enumerate(summary_documents)]\n",
    "response = pinecone_index.upsert(vectors=to_upload, namespace=\"summary-pointers\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we find a match, we're going to substitute the original source text in our prompt to answer the user's question.\n",
    "\n",
    "Let's define a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_original_context(response):\n",
    "  context = \"\"\n",
    "  for match in response['matches']:\n",
    "    context += large_chunks[int(match['metadata']['source_id'])].page_content + \"\\n\\n\"\n",
    "  return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-run our vector search against our original query and see what comes back. \n",
    "\n",
    "First Query: `\"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt: ***************************\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-------------------------------------\n",
      "So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense.\n",
      "\n",
      "Step one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now.\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Helpful Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPT-4 gave nonsense answers in the examples of determining the time it takes to dry 30 clothes based on the time it took to dry 5 clothes, and in the method suggested to measure 6 liters of water using a 12-liter jug and a 6-liter jug. Additionally, GPT-4 provided a wrong answer regarding the likelihood of getting a flat tire while bicycling over a bridge suspended over nails, screws, and broken glass.'"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=query_emb, namespace=\"summary-pointers\", top_k=1, include_metadata=True)\n",
    "\n",
    "run_llm_qa_prompt(context=retrieve_original_context(response), question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only is that the right answer, it's well reasoned.\n",
    "\n",
    "Let's see how it does on our more detailed follow up question.\n",
    "\n",
    "`\"Explain the example where GPT-4 failed to reason about drying clothes.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt: ***************************\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-------------------------------------\n",
      "So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense.\n",
      "\n",
      "Step one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now.\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Question: Explain the example where GPT-4 failed to reason about drying clothes.\n",
      "Helpful Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the example where GPT-4 failed to reason about drying clothes, the context states that five clothes took five hours to dry completely in the sun. The question then asks how long it would take to dry 30 clothes. GPT-4 incorrectly responds with 30 hours, which is not a logical answer.'"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=follow_up_query_emb, namespace=\"summary-pointers\", top_k=1, include_metadata=True)\n",
    "\n",
    "run_llm_qa_prompt(context=retrieve_original_context(response), question=follow_up_query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not only are we getting better output, we're requiring significantly less vector storage to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selectively run these as needed to clean up and start a section over, or to remove the index completely when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.delete(delete_all=True, namespace=\"direct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.delete(delete_all=True, namespace=\"summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.delete(delete_all=True, namespace=\"summary-pointers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(\"ted-talk-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
