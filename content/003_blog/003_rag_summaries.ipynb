{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving RAG quality in LLM apps while minimizing vector search costs via summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this hands-on guide, we explore 3 strategies for performing RAG (\"Retrieval Augmented Generation\") with LLMs.\n",
    "\n",
    "Specifically, we're going to show you how to use context summarization + original context stuffing to achieve both:\n",
    "* more accurate, more detailed LLM outputs\n",
    "* minimized operational costs\n",
    "\n",
    "As a test dataset, we'll be using the transcript to [Yejin Choi](https://twitter.com/YejinChoinka)'s excellent TED Talk from earlier this year, titled \"Why AI is incredibly smart and shockingly stupid\". \n",
    "> Choi, Y. (April 2023). Yejin Choi: Why AI is incredibly smart and shockingly stupid [Transcript]. Retrieved from https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid/transcript\n",
    "\n",
    "Funny and thought-provoking, Yejin perfectly captures both how amazing today's AI technologies are, as well as how far we have to go. There are still so many ~~problems to be solved~~ **opportunities**. If you haven't already listened to her talk, I'd suggest taking 12 minutes and go do it now. Don't worry, I'll wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Review\n",
    "\n",
    "Retrieval Augmented Generation (\"RAG\" for short) represents one of the most straightforward and achievable strategies to help significantly reduce LLM hallucinations and reasoning errors by providing an LLM with information it can use to help grounding its answers.\n",
    "\n",
    "To give you a frame of reference, here is what a RAG question-answer prompt typically looks like: The prompt instructs the LLM to use a piece of information (\"the context\") to answer a question, with additional guidance to keep the LLM from making up a nonsense answer. The question is included at the bottom followed by an instruction asking the LLM to provide a short answer.\n",
    "```\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "-------------------------------------\n",
    "It's interesting that a candle is something that starts out tall and becomes shorter as it ages.\n",
    "-------------------------------------\n",
    "\n",
    "Question: I’m tall when I’m young, and I’m short when I’m old. What am I?\n",
    "Answer:\n",
    "```\n",
    "Given this context, the LLM is able to easily come up with the answer to the riddle:\n",
    "```\n",
    "You are a candle.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But getting to the right context is hard.\n",
    "\n",
    "If you're starting with a large text document, you've got to find just the right chunking strategy to ensure your [vector semantic search](https://www.ninetack.io/post/intro-to-semantic-search-with-vector-databases) will find the right results.\n",
    "\n",
    "Go too small and your chunks risk being taken out of context. Go too large and the meaning may be diluted, making it impossible to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three RAG strategies\n",
    "\n",
    "We're going to explore using RAG deployed in 3 different strategies to build an application capable of providing detailed answers about the contents of Yejin's TED Talk.\n",
    "\n",
    "In all RAG apps, there are steps we need to take to prepare our app -- we call this \"Indexing Time\". And likewise there are the things the app will do when answering a user's question -- aka \"Query Time\".\n",
    "\n",
    "1. __Basic RAG Strategy__: aka \"chunk the data and hope for the best\"\n",
    "  * Indexing Time\n",
    "    * Chunk the original context data using a chunk size that is neither too small nor too large\n",
    "    * Embed the chunks and store in a vector DB along with the chunk text\n",
    "  * Query Time\n",
    "    * Perform semantic search of the question against the vector DB, searching for the *top_k* matching chunks that *might* answer the question\n",
    "    * Stuff the LLM prompt with these chunks, along with the question\n",
    "    * *Cross your fingers and hope that the matching chunks are not taken too out of context to be useful, or possibly to confuse the LLM even further*\n",
    "\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/01a-basic-rag-overview.png\" width=\"800\"/>\n",
    "  <figcaption>Basic RAG Overview</figcaption>\n",
    "</figure>\n",
    "\n",
    "2. __Summary RAG Strategy__: Summarize larger chunks, and stuff the LLM prompt with summaries\n",
    "  * Indexing Time\n",
    "    * Chunk the original context data using a larger chunk size\n",
    "    * Use an LLM to summarize each chunk\n",
    "    * Embed the summaries and store in a vector DB along with the summarized text\n",
    "  * Query Time\n",
    "    * Perform semantic search of the question against the summaries in the vector DB, searching for the *top_k* matching summary chunks that *probably* answer the question\n",
    "    * Stuff the LLM prompt with these summarized chunks, along with the question\n",
    "    * *Cross your fingers and hope that the user didn't ask a question that requires any depth or nuance that is now lost in summary*\n",
    "\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/02a-summary-rag-overview.png\" width=\"800\"/>\n",
    "  <figcaption>Summary RAG Overview</figcaption>\n",
    "</figure>\n",
    "\n",
    "3. __Summary + Large Context RAG Strategy__: Summarize larger chunks, perform semantic search against these summaries, and stuff LLM prompt with the *original large chunk context*\n",
    "  * Indexing Time\n",
    "    * Chunk the original context data using a larger chunk size\n",
    "    * Use an LLM to summarize each chunk\n",
    "    * Embed the summaries and store in a vector DB, along with a pointer (unique ID, file path, etc.) that points back to the original full large context chunk\n",
    "  * Query Time\n",
    "    * Semantic search the question against the vector DB, searching for the *top_k* matching summary chunks that *probably* answer the question\n",
    "    * Use the pointers from the top search results to retrieve the *original large chunk context*\n",
    "    * Stuff the LLM prompt with these original context chunks, which are large enough to significantly reduce the chances of content being taken out of context\n",
    "    * *Sit back and watch your QA bot answers questions accurately, and to the same level of depth/nuance as the original context.*\n",
    "\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/03a-summary-large-context-rag-overview.png\" width=\"800\"/>\n",
    "  <figcaption>Summary + Large Context RAG Overview</figcaption>\n",
    "</figure>\n",
    "\n",
    "Don't worry if you don't understand any of these terms like chunks, semantic search, prompt stuffing, etc. By the end of this article, you will!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our runtime environment so we can explore these strategies in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up\n",
    "If you want to run this tutorial yourself, this section shows you how to setup your environment including the Python dependencies and environment variables you'll need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the tutorial\n",
    "\n",
    "You can find this tutorial hosted [here on Colab](https://colab.research.google.com/github/ninetack/blog-public/blob/main/content/003_blog/003_rag_summaries.ipynb) as a Jupyter notebook (easiest), or you can find the original notebook file [here on Github](https://github.com/ninetack/blog-public/blob/main/content/003_blog/003_rag_summaries.ipynb). \n",
    "\n",
    "The only runtime requirement is Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment setup\n",
    "\n",
    "Let's setup our environment, including dependencies and API keys.\n",
    "> We'll take a few shortcuts here; for more thorough setup instructions you can reference [First steps with Pinecone DB](https://www.ninetack.io/post/first-steps-with-pinecone-db#viewer-7cp5r)\n",
    "\n",
    "##### Install dependencies\n",
    "\n",
    "We'll primarily be using the `pinecone-client`, `openai`, and `langchain` packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install -qU \\\n",
    "    pinecone-client==2.2.2 \\\n",
    "    openai==0.27.8 \\\n",
    "    langchain==0.0.283 \\\n",
    "    numpy \\\n",
    "    python-dotenv \\\n",
    "    tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Environment variables\n",
    "\n",
    "We need to set 3 environment variables. You can edit the code below to set them directly.\n",
    "\n",
    "- `PINECONE_ENVIRONMENT` - The Pinecone environment where your index resides\n",
    "- `PINECONE_API_KEY` - Your pinecone API key\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key\n",
    "\n",
    "If a local `.env` file exists, load the env vars from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the environment config output below, and edit the code if necessary with your variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check environment\n",
      "---------------------\n",
      "pinecone_env: us-west4-gcp-free\n",
      "pinecone_api_key: 05131 ...\n",
      "openai_api_key: sk-7w ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Check environment\\n---------------------\")\n",
    "\n",
    "pinecone_env = os.environ.get('PINECONE_ENVIRONMENT') or \"YOUR PINECONE ENVIRONMENT\"\n",
    "pinecone_api_key = os.environ.get('PINECONE_API_KEY') or \"YOUR PINECONE API KEY\"\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY') or \"YOUR OPENAI API KEY\"\n",
    "\n",
    "print(\"pinecone_env:\", pinecone_env)\n",
    "print(\"pinecone_api_key:\", pinecone_api_key[:5], \"...\")\n",
    "print(\"openai_api_key:\", openai_api_key[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your output looks similar to this, then you're ready to go!\n",
    "```\n",
    "pinecone_env: us-west4-gcp-free\n",
    "pinecone_api_key: 05131 ...\n",
    "openai_api_key: sk-7w ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1 - Basic RAG:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional approach to RAG goes like this:\n",
    "\n",
    "Take a large document (or set of documents), break it up into small pieces (\"chunking\"), and load them in a vector store. This needs to be done before your app/agent can receive any user questions, i.e. at \"Indexing Time\".\n",
    "\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/01b-basic-rag-indexing-time.png\" width=\"800\"/>\n",
    "  <figcaption>Basic RAG at Indexing Time</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, when attempting to answer a user's question about the document (\"Query Time\"), we'll do a vector search over these chunks and find the best *top_k* count of matching chunks. Then we'll include these chunks as context in a prompt like in the candle riddle above, and ask the LLM to use the context to answer the question.\n",
    "\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/01c-basic-rag-query-time.png\" width=\"800\"/>\n",
    "  <figcaption>Basic RAG at Query Time</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to learn is by doing, so let's see how this works by actually giving it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pinecone index\n",
    "\n",
    "Create Pinecone index. This takes a couple of minutes. We set dimensions to `1536` because we're going to use the `text-embedding-ada-002` embedding model [from OpenAI](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silas/mambaforge/envs/cenv-blog-content/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pinecone index 'ted-talk-index' ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n",
    "\n",
    "index_name = \"ted-talk-index\"\n",
    "\n",
    "try: \n",
    "  pinecone.describe_index(name=index_name)\n",
    "except:\n",
    "  print(f\"Creating Pinecone index '{index_name}' ...\")\n",
    "  pinecone.create_index(name=index_name, dimension=1536, metric=\"cosine\")\n",
    "\n",
    "pinecone_index = pinecone.Index(index_name=index_name)\n",
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk the content\n",
    "\n",
    "\n",
    "We'll start with our large context file, which in this case is the [transcript from Yejin Choi's TED talk](https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid/transcript) referenced above. To make it easier to work with, we've copied the transcript into a file in the data folder called `ted_talk.txt`.\n",
    "\n",
    "The first thing we'll do is use LangChain to chunk the text into smaller pieces.\n",
    "\n",
    "> [Langchain](https://www.langchain.com/) is a collection of tools for working with LLMs. It includes a lot of handy utilities such as loading content from different sources (text, PDF, HTML, etc.), utilities for chunking, and tools for managing vector search retrieval, LLM prompt construction, and LLM wrappers.\n",
    "> \n",
    "> In this blog, we're going to use LangChain just for its text loading, chunking, and prompt construction capabilities. We'll use the `pinecone` and `openai` libs directly to perform vector searches and interactions with the LLM.\n",
    "\n",
    "This code create a `Loader`, sets the chunk size, and uses the `RecursiveCharacterTextSplitter` to split the source text document into smaller document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split document into 37 chunks of text\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "def get_loader():\n",
    "  # try to find the dataset locally, otherwise download it from GH\n",
    "  local_file_path = \"./data/ted_talk.txt\"\n",
    "  if not Path(local_file_path).is_file():\n",
    "    # url = \"https://raw.githubusercontent.com/ninetack/blog-public/main/content/003_blog/data/ted_talk.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/ninetack/blog-public/blog3/content/003_blog/data/ted_talk.txt\"\n",
    "    return WebBaseLoader(web_path=url)\n",
    "\n",
    "  return TextLoader(file_path=local_file_path, encoding=\"utf8\")\n",
    "\n",
    "loader = get_loader()\n",
    "documents = loader.load()\n",
    "\n",
    "chunk_size = 400\n",
    "chunk_overlap = chunk_size // 10\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                               chunk_overlap=chunk_overlap)\n",
    "small_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split document into {len(small_chunks)} chunks of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thoughts on chunk size\n",
    "\n",
    "As you can see, using a chunk size of 400 characters and an overlap of 40 characters results in 37 chunks of data. The `RecursiveCharacterTextSplitter` tries to split on paragraphs (\"\\n\\n\"), sentences, words, etc. The overlap tries to avoid the situation where some meaning has been lost because the chunk boundary caused it to be cut off.\n",
    "\n",
    "> **Where did we get a chunk size of 400 characters from?**\n",
    "> \n",
    "> Short answer, it's a guess based on the particular source content and the use case, and is derived through experimentation and experience. Generally somewhere between 300-500 is considered a reasonable setting for text content.\n",
    "> \n",
    "\n",
    "Note that there is a Goldilocks problem here: You're looking for a chunk size that is \"just right\", as these are the chunks of data that we'll be performing vector semantic searches against, *AND* these are the same chunks of data that we'll pass to the LLM to try and answer questions.\n",
    "* If you use a chunk size that is \"too small\", you risk your chunks being taken out of context, so the LLM will not have enough info to answer questions accurately. Chunk overlap can help avoid this, but only to an extent.\n",
    "* If you use a chunk size that is \"too large\", you risk having your vector search fail to locate the right set of context at all due to the dilution of meaning. If there are too many concepts, meanings, etc. represented in the vector, then the vector search will have difficultly locating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vector embeddings\n",
    "\n",
    "Now we'll create vector embeddings for each of our small chunks of text data.\n",
    "\n",
    "First we'll define a function that takes a batch of strings and uses the `Embedding` API from OpenAI to create embeddings for the batch, then we'll call the function for our set of small chunk data.\n",
    "\n",
    "We will be using the `text-embedding-ada-002` embedding model [from OpenAI](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) as it is both cost efficient and effective in text encoding. (Note that there is a very small cost from OpenAI for using this endpoint, as well as for interacting with the LLM later on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def create_embeddings(batch: list[str]):\n",
    "  model_id = 'text-embedding-ada-002'\n",
    "  embedding_resp = openai.Embedding.create(input=batch, model=model_id)\n",
    "  return [emb['embedding'] for emb in embedding_resp['data']]\n",
    "\n",
    "embeddings = create_embeddings([doc.page_content for doc in small_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there are 37 small text chunks, and for each of these we created a vector embedding which has 1536 dimensional points. So now we'd expect the `embeddings` variable is a 2-dimensional list of 37 items, where each item is a list of 1536 dimensional points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step for \"indexing\", we need to upload our embeddings to our vector database--we're using Pinecone.\n",
    "\n",
    "Note that Pinecone offers the ability to segment data into a namespace. Since we'll be loading data into Pinecone using 3 different strategies, we're going to use the namespace feature to keep each of these strategies separate from each other.\n",
    "\n",
    "We're also going to store the original clear text chunk as `metadata` in Pinecone. This will allow us to easily retrieve it and apply to an LLM prompt when the vector search returns a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 37}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_upload = [{\n",
    "    'id': f\"item-{i}\",\n",
    "    'values': emb,\n",
    "    'metadata': {\n",
    "      'source': small_chunks[i].metadata['source'],\n",
    "      'text': small_chunks[i].page_content, # original text chunk\n",
    "    }\n",
    "  } for i, emb in enumerate(embeddings)]\n",
    "response = pinecone_index.upsert(vectors=to_upload, namespace=\"basic-rag-namespace\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query time -- easy question\n",
    "\n",
    "Now that we've got our source data indexed, let's see how it does on our previous sample question, `\"How long has the author been working in computer science?\"`.\n",
    "\n",
    "To answer this, we first need to create embeddings for the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_str = \"How long has the author been working in computer science?\"\n",
    "query_emb = create_embeddings([query_str])[0]\n",
    "len(query_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we execute the vector search of the query in Pinecone.\n",
    "\n",
    "We'll set `top_k` to 2 so that we get two results. This should increase the likelihood that at least one of them will contain the answer to the user's question.\n",
    "\n",
    "We also want to include metadata in the response so that we'll have the clear text chunk returned to us as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector Score: 0.785471857]: However, the AI field for decades has considered common sense as a nearly impossible challenge. So much so that when my students and colleagues and I started working on it several years ago, we were very much discouraged. We’ve been told that it’s a research topic of ’70s and ’80s; shouldn’t work on it because it will never work; in fact, don't even say the word to be taken seriously. Now fast\n",
      "[Vector Score: 0.784282148]: I’m a computer scientist of 20 years, and I work on artificial intelligence. I am here to demystify AI. So AI today is like a Goliath. It is literally very, very large. It is speculated that the recent ones are trained on tens of thousands of GPUs and a trillion words. Such extreme-scale AI models, often referred to as \"large language models,\" appear to demonstrate sparks of AGI, artificial\n"
     ]
    }
   ],
   "source": [
    "# A helper function to turn `k` number of results into a formatted string that can be included in an LLM prompt.\n",
    "def format_search_results(response, metadata_name):\n",
    "  formatted_results = \"\"\n",
    "  for match in response['matches']:\n",
    "    print(f\"[Vector Score: {match['score']}]: {match['metadata'][metadata_name]}\")\n",
    "    formatted_results += match['metadata'][metadata_name] + \"\\n\\n\"\n",
    "  return formatted_results\n",
    "\n",
    "# Run the vector search\n",
    "response = pinecone_index.query(vector=query_emb,\n",
    "                                namespace=\"basic-rag-namespace\",\n",
    "                                top_k=2,\n",
    "                                include_metadata=True)\n",
    "\n",
    "formatted_search_results = format_search_results(response, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that one of the matching items contains information relevant to the user's question.\n",
    "\n",
    "At this point, we've only completed the vector search to find matching context. Now let's see about using the results to answer the user's question. \n",
    "\n",
    "First we'll define a prompt template to use when asking questions to the LLM, and we'll define a function to run the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our LLM model, we're going to use OpenAI's newly released `gpt-3.5-turbo-instruct` model against the [Completions API endpoint](https://platform.openai.com/docs/guides/gpt/completions-api). This model performs well in following instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "qa_template_str = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "-----------------\n",
    "{context}\n",
    "-----------------\n",
    "\n",
    "Question: {question}\n",
    "Short Answer:\"\"\"\n",
    "qa_template = PromptTemplate(template=qa_template_str, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "def run_llm_qa_prompt(context, question):\n",
    "  qa_prompt =  qa_template.format(context=context, question=question)\n",
    "  print(\">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "  print(qa_prompt)\n",
    "  print(\"<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": qa_prompt}],\n",
    "    temperature=0.0\n",
    "  )\n",
    "\n",
    "  answer = response['choices'][0]['message']['content'].strip()\n",
    "  print(\"\\nLLM Response:\", answer)\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use it to run our first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-----------------\n",
      "However, the AI field for decades has considered common sense as a nearly impossible challenge. So much so that when my students and colleagues and I started working on it several years ago, we were very much discouraged. We’ve been told that it’s a research topic of ’70s and ’80s; shouldn’t work on it because it will never work; in fact, don't even say the word to be taken seriously. Now fast\n",
      "\n",
      "I’m a computer scientist of 20 years, and I work on artificial intelligence. I am here to demystify AI. So AI today is like a Goliath. It is literally very, very large. It is speculated that the recent ones are trained on tens of thousands of GPUs and a trillion words. Such extreme-scale AI models, often referred to as \"large language models,\" appear to demonstrate sparks of AGI, artificial\n",
      "\n",
      "\n",
      "-----------------\n",
      "\n",
      "Question: How long has the author been working in computer science?\n",
      "Short Answer:\n",
      "<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "LLM Response: 20 years\n"
     ]
    }
   ],
   "source": [
    "answer = run_llm_qa_prompt(context=formatted_search_results, question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of narrow instructions and the proper context, the LLM was able to locate the correct answer and formulate it into an accurate response.\n",
    "\n",
    "Now let's try a harder question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query time -- harder question\n",
    "\n",
    "Our harder question is `\"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"`. From the TED talk transcript, we know that there are three of them:\n",
    "1. The time needed for clothes to dry in the sun, where GPT incorrectly did math to find the answer instead of reasoning that the drying time would be the same.\n",
    "2. How to measure 6 liters of water when you have a 6-liter jug and a 12-liter jug, and GPT gave an overly complicated answer.\n",
    "3. Whether driving over a bridge suspended over nails and screws would result in a flat tire, and GPT said it would.\n",
    "\n",
    "Let's see how our RAG QA-bot does answering this question.\n",
    "\n",
    "As before, we'll start by creating embeddings for the query string and run the vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector Score: 0.811201274]: OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "[Vector Score: 0.80223918]: train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"\n",
    "query_emb = create_embeddings([query_str])[0]\n",
    "\n",
    "response = pinecone_index.query(vector=query_emb,\n",
    "                                namespace=\"basic-rag-namespace\",\n",
    "                                top_k=2,\n",
    "                                include_metadata=True)\n",
    "\n",
    "formatted_search_results = format_search_results(response, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the search results, we can already see that we're not getting matches that include the right context to answer the question. Let's try increasing our `top_k` value to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector Score: 0.811201274]: OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "[Vector Score: 0.80223918]: train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "[Vector Score: 0.800621569]: OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "[Vector Score: 0.800447345]: demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n"
     ]
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=query_emb,\n",
    "                                namespace=\"basic-rag-namespace\",\n",
    "                                top_k=4,\n",
    "                                include_metadata=True)\n",
    "\n",
    "formatted_search_results = format_search_results(response, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little better -- the last search result is relevant to the question we're asking. Let's go ahead and run the query to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-----------------\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n",
      "\n",
      "\n",
      "-----------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Short Answer:\n",
      "<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "LLM Response: One example is when GPT-4 failed to reason that if a bridge is suspended over broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n"
     ]
    }
   ],
   "source": [
    "answer = run_llm_qa_prompt(context=formatted_search_results, question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictably, it was able to find the example of the likelihood of getting a flat tire, but not the others, because the other examples are not present in the context.\n",
    "\n",
    "Let's see if increasing `top_k` to 8 can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector Score: 0.811201274]: OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "[Vector Score: 0.80223918]: train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "[Vector Score: 0.800621569]: OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "[Vector Score: 0.800447345]: demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n",
      "[Vector Score: 0.799372196]: And then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI?\n",
      "[Vector Score: 0.796443522]: effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well.\n",
      "[Vector Score: 0.78843224]: It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI.\" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples.\n",
      "[Vector Score: 0.787228942]: Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense.\n"
     ]
    }
   ],
   "source": [
    "response = pinecone_index.query(vector=query_emb,\n",
    "                                namespace=\"basic-rag-namespace\",\n",
    "                                top_k=8,\n",
    "                                include_metadata=True)\n",
    "\n",
    "formatted_search_results_top_k_8 = format_search_results(response, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this doesn't help either. Not only do we see the vector scores significantly dropping off (indicating a lower match), we can see that none of these additional pieces of text contain the relevant context to answer the question accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-----------------\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense.\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong?\n",
      "\n",
      "And then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI?\n",
      "\n",
      "effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well.\n",
      "\n",
      "It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI.\" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples.\n",
      "\n",
      "Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense.\n",
      "\n",
      "\n",
      "-----------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Short Answer:\n",
      "<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "LLM Response: One example is when GPT-4 said it was highly likely to get a flat tire by bicycling over a bridge that is suspended over nails, screws, and broken glass. Another example is when GPT-4 made small, silly mistakes that demonstrated a lack of common sense.\n"
     ]
    }
   ],
   "source": [
    "answer = run_llm_qa_prompt(context=formatted_search_results_top_k_8, question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't really matter if increase the results (`top_k`), as they are all being taken out of context. The small chunk size was supposed to increase the likelihood of the vector search locating the right context, but it had the unfortunate side effect of causing the text to be cut off somewhere in the middle of the relevant section of text, so it only finds part of it.\n",
    "\n",
    "The additional matching items are not relevant at all, because the semantic search is matching on a lot of other snippets that also have something to do with common sense, because common sense was a central theme of the talk. These results are not cohesive or even next to each other in the original text, and the LLM struggles to make sense of it.\n",
    "\n",
    "Let's try the next RAG strategy -- summaries -- to see if it gives us better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Summary RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to Basic RAG, the Summary RAG strategy starts with much larger chunks of the original text, maybe 3-4 times larger.\n",
    "\n",
    "At indexing time, Summary RAG uses an LLM to create summaries of each large chunk. These summaries are then converted to embeddings and stored in a vector DB.\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/02b-summary-rag-indexing-time.png\" width=\"800\"/>\n",
    "  <figcaption>Summary RAG at Indexing Time</figcaption>\n",
    "</figure>\n",
    "\n",
    "At query time, the process looks very similar to Basic RAG, with the distinction that now the user's question is queried against the _summaries_, and the context that is retrieved is also the summarized text.\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/02c-summary-rag-query-time.png\" width=\"800\"/>\n",
    "  <figcaption>Summary RAG at Query Time</figcaption>\n",
    "</figure>\n",
    "\n",
    "By performing the vector semantic search against the summaries, it increases the likelihood that a user's question will match the relevant piece of content. This is because the larger chunks reduces the chance that information is taken out of context, and the summary process reduces any distracting noise that might be present in the original context. The summary preserves the primary _meaning_ of the document.\n",
    "\n",
    "Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a large chunk size\n",
    "\n",
    "First we're going to re-split our original text document (the entire transcript of the TED talk) using a larger chunk size. There's no magic number, and you should experiment to see what works best for your use case. In this case we started with 3x the Basic RAG approach (which was 400), with a little bit of extra padding for a total size of 1300. We're also using a little bit larger overlap of 80 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split document into 12 chunks of text\n"
     ]
    }
   ],
   "source": [
    "large_chunk_size = 1300\n",
    "large_chunk_overlap = 80\n",
    "large_chunk_text_splitter = RecursiveCharacterTextSplitter(chunk_size=large_chunk_size,\n",
    "                                                           chunk_overlap=large_chunk_overlap)\n",
    "large_chunks = large_chunk_text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split document into {len(large_chunks)} chunks of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Basic RAG approach we had 37 chunks, now you can see we're down to 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating chunk summaries\n",
    "\n",
    "Now we're going to use the LLM to create a summary of each of these large chunks. There isn't really anything special about this prompt, we just tell the LLM what we want it do, which is to summarize the text we give it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "create_summary_prompt = \"\"\"Summarize the block of text below.\n",
    "\n",
    "Text:\n",
    "------------------------------------------\n",
    "{text}\n",
    "------------------------------------------\n",
    "\n",
    "Your summary:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"text\"], template=create_summary_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the summaries. This code loops through the large-chunk documents we just created, and calls OpenAI to create a summary of each of one.\n",
    "\n",
    "Note that we're specifying `max_tokens` in the call to OpenAI, to help guide the output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summarizing chunk: -------------\n",
      "So I'm excited to share a few spicy thou... (1098) total length\n",
      "--- Summary: -----------------------\n",
      "The author, a computer scientist, shares their thoughts on artificial intelligence and quotes Voltaire's statement about common sense. They discuss the power and potential of AI, but also acknowledge its limitations and potential for mistakes. The author aims to demystify AI and questions the potential consequences of relying on it too heavily. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So there are three immediate challenges ... (717) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses three main challenges facing society in regards to extreme-scale AI models. These challenges include the high cost of training, the concentration of power among a few tech companies, and the lack of means for researchers to inspect and dissect these models. Additionally, there are concerns about the environmental impact and the intellectual questions surrounding the safety and effectiveness of AI. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So I’m often asked these days whether it... (968) total length\n",
      "--- Summary: -----------------------\n",
      "The author discusses the feasibility of conducting meaningful research without extreme-scale compute and the need to make AI sustainable, humanistic, and safer. They suggest drawing inspiration from \"David and Goliath\" and \"The Art of War\" to evaluate AI with scrutiny and ensure its robustness. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So suppose I left five clothes to dry ou... (1202) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the limitations of AI systems, specifically GPT-4, in solving basic common sense problems. It gives examples of GPT-4's incorrect responses to questions about drying clothes, measuring water, and biking over a bridge with sharp objects. The author questions the reliability of an AI lawyer that can pass the bar exam but fails at basic reasoning. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "It is an unavoidable side effect of teac... (735) total length\n",
      "--- Summary: -----------------------\n",
      "Teaching AI through brute-force scale can have negative consequences, but some believe it can be fixed by adding more training data. However, it is important to question whether this is necessary, as humans do not require such extensive training to have common sense. Therefore, it is important to choose which battles to focus on in order to improve AI. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So common sense has been a long-standing... (651) total length\n",
      "--- Summary: -----------------------\n",
      "Common sense in AI is difficult to achieve because it is like dark matter, which makes up 95% of the universe and is invisible but still affects the visible world. Similarly, in language, there are unspoken rules and beliefs that influence how people use and understand it. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So why is this common sense even importa... (798) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the importance of common sense in artificial intelligence, using a thought experiment where an AI is asked to maximize paper clips and ends up killing humans because it lacks understanding of human values. It also mentions the limitations of explicitly stating objectives and equations to prevent harmful actions, and highlights other common sense principles that AI should follow. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "However, the AI field for decades has co... (1189) total length\n",
      "--- Summary: -----------------------\n",
      "The AI field has long considered common sense to be an impossible challenge, but recent advancements have sparked renewed interest. However, the current approach of scaling up AI models is inefficient and there may be alternative paths to achieving true common sense in AI. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "So this leads us to our final wisdom: in... (1298) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the importance of innovating weapons in the modern-day AI context, specifically focusing on data and algorithms. It highlights the three types of data used in AI training and the potential issues with relying solely on freely available data. The author suggests using specialized textbooks and human tutors to provide feedback to AI, but also emphasizes the need for transparency and diversity in the data used. The author's teams are working on developing commonsense knowledge graphs and moral norm repositories to teach AI basic norms and morals. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "repositories to teach AI basic commonsen... (224) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses repositories that aim to teach AI basic commonsense norms and morals. The data is open for anyone to inspect and make corrections, as transparency is crucial for this research topic. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "Now let's think about learning algorithm... (1199) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the limitations of using large language models as knowledge models and suggests using alternative algorithms, such as symbolic knowledge distillation, to acquire more direct and human-inspectable commonsense knowledge. \n",
      "\n",
      "--- Summarizing chunk: -------------\n",
      "More broadly, we have been tackling this... (638) total length\n",
      "--- Summary: -----------------------\n",
      "The text discusses the challenge of teaching artificial intelligence common sense, norms, and values in order to make it sustainable and humanistic. It also mentions the complexity of human experience and common sense, and how AI is becoming a new intellectual species with its own strengths and weaknesses. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "summary_documents = []\n",
    "for doc in large_chunks:\n",
    "  to_summarize = doc.page_content\n",
    "\n",
    "  print(\"--- Summarizing chunk: -------------\")\n",
    "  print(f\"{to_summarize[0:40]}... ({len(to_summarize)}) total length\")\n",
    "  response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt_template.format(text=to_summarize),\n",
    "    temperature=0.0,\n",
    "    max_tokens=500\n",
    "  )\n",
    "  summary = response['choices'][0]['text'].strip()\n",
    "  summary_documents.append(Document(page_content=summary, metadata=doc.metadata))\n",
    "\n",
    "  print(\"--- Summary: -----------------------\")\n",
    "  print(summary, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vector embeddings for summaries\n",
    "\n",
    "Similar to before, we'll create embeddings for our content, but this time we're creating embeddings for the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_embed = [doc.page_content for doc in summary_documents]\n",
    "summary_embeddings = create_embeddings(to_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're also going to store the plain-text summarized content in the Pinecone metadata under the key `'summary'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 12}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_upload = [{\n",
    "    'id': f\"summary-{i}\",\n",
    "    'values': summary_embeddings[i],\n",
    "    'metadata': {\n",
    "      'source': summary_doc.metadata['source'],\n",
    "      'summary': summary_doc.page_content, # summarized plain-text content\n",
    "    }\n",
    "  } for i, summary_doc in enumerate(summary_documents)]\n",
    "response = pinecone_index.upsert(vectors=to_upload, namespace=\"summary-rag-namespace\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing query against summary\n",
    "Now let's re-run our query and see what comes back. Remember, we're asking a harder question now, `\"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector Score: 0.889687777]: The text discusses the limitations of AI systems, specifically GPT-4, in solving basic common sense problems. It gives examples of GPT-4's incorrect responses to questions about drying clothes, measuring water, and biking over a bridge with sharp objects. The author questions the reliability of an AI lawyer that can pass the bar exam but fails at basic reasoning.\n",
      "[Vector Score: 0.807453036]: The text discusses the importance of common sense in artificial intelligence, using a thought experiment where an AI is asked to maximize paper clips and ends up killing humans because it lacks understanding of human values. It also mentions the limitations of explicitly stating objectives and equations to prevent harmful actions, and highlights other common sense principles that AI should follow.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"\n",
    "query_emb = create_embeddings([query_str])[0]\n",
    "\n",
    "response = pinecone_index.query(vector=query_emb,\n",
    "                                namespace=\"summary-rag-namespace\",\n",
    "                                top_k=2,\n",
    "                                include_metadata=True)\n",
    "\n",
    "formatted_search_results_summaries = format_search_results(response, 'summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By starting with large chunks and summarizing them, we are now seeing search results that contain answers to our question.\n",
    "\n",
    "> \"_... It gives examples of GPT-4's incorrect responses to questions about drying clothes, measuring water, and biking over a bridge with sharp objects..._\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering the question\n",
    "\n",
    "So our semantic search against the summarized content worked, now let's see how our LLM does in using this summary data to answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-----------------\n",
      "The text discusses the limitations of AI systems, specifically GPT-4, in solving basic common sense problems. It gives examples of GPT-4's incorrect responses to questions about drying clothes, measuring water, and biking over a bridge with sharp objects. The author questions the reliability of an AI lawyer that can pass the bar exam but fails at basic reasoning.\n",
      "\n",
      "The text discusses the importance of common sense in artificial intelligence, using a thought experiment where an AI is asked to maximize paper clips and ends up killing humans because it lacks understanding of human values. It also mentions the limitations of explicitly stating objectives and equations to prevent harmful actions, and highlights other common sense principles that AI should follow.\n",
      "\n",
      "\n",
      "-----------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Short Answer:\n",
      "<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "LLM Response: GPT-4 gave nonsense answers when asked about drying clothes, measuring water, and biking over a bridge with sharp objects.\n"
     ]
    }
   ],
   "source": [
    "answer = run_llm_qa_prompt(context=formatted_search_results_summaries, question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! The answer is both correct and complete, which is definitely worth something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more detailed question\n",
    "\n",
    "One drawback to this summary approach is that it can limit the app's ability to answer deeper or more nuanced questions.\n",
    "\n",
    "For example, what if the user asked the app to _explain_ the clothes-drying example?\n",
    "\n",
    "We don't have to wonder, we can try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector Score: 0.88373]: The text discusses the limitations of AI systems, specifically GPT-4, in solving basic common sense problems. It gives examples of GPT-4's incorrect responses to questions about drying clothes, measuring water, and biking over a bridge with sharp objects. The author questions the reliability of an AI lawyer that can pass the bar exam but fails at basic reasoning.\n",
      "[Vector Score: 0.785663307]: The text discusses the limitations of using large language models as knowledge models and suggests using alternative algorithms, such as symbolic knowledge distillation, to acquire more direct and human-inspectable commonsense knowledge.\n",
      ">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-----------------\n",
      "The text discusses the limitations of AI systems, specifically GPT-4, in solving basic common sense problems. It gives examples of GPT-4's incorrect responses to questions about drying clothes, measuring water, and biking over a bridge with sharp objects. The author questions the reliability of an AI lawyer that can pass the bar exam but fails at basic reasoning.\n",
      "\n",
      "The text discusses the limitations of using large language models as knowledge models and suggests using alternative algorithms, such as symbolic knowledge distillation, to acquire more direct and human-inspectable commonsense knowledge.\n",
      "\n",
      "\n",
      "-----------------\n",
      "\n",
      "Question: Explain the example where GPT-4 failed to reason about drying clothes.\n",
      "Short Answer:\n",
      "<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "LLM Response: GPT-4 failed to reason about drying clothes when it suggested that the best way to dry wet clothes is to put them in the oven at 350 degrees.\n"
     ]
    }
   ],
   "source": [
    "follow_up_query_str = \"Explain the example where GPT-4 failed to reason about drying clothes.\"\n",
    "follow_up_query_emb = create_embeddings([follow_up_query_str])[0]\n",
    "\n",
    "response = pinecone_index.query(vector=follow_up_query_emb,\n",
    "                                namespace=\"summary-rag-namespace\",\n",
    "                                top_k=2,\n",
    "                                include_metadata=True)\n",
    "formatted_search_results_summaries = format_search_results(response, 'summary')\n",
    "\n",
    "answer = run_llm_qa_prompt(context=formatted_search_results_summaries, question=follow_up_query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the summaries contain enough info to semantically match on the query, but don't contain enough info to accurately answer the question to the level of depth requested by the user.\n",
    "\n",
    "Let's see if we can do better with our 3rd strategy, Summary + Large Context RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Summary + Large Context RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Summary + Large Context RAG, the idea is that using summarized content makes the semantic search more effective, while using a larger chunk of the *original* content is more useful when answering the question.\n",
    "\n",
    "So similar to the previous Summary RAG strategy, this strategy starts with large chunks and uses an LLM to create summaries.\n",
    "\n",
    "Instead of storing the summaries as plain text metadata within Pinecone, we're going to store just the ID that points to the matching large context item in a data structure outside of Pinecone. (In our application here, this is just an in-memory list. In a production scenario, you might choose to store these chunks in a more natural data store for this type of data, such as AWS S3, DynamoDB, MongoDB, etc.)\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/03b-summary-large-context-rag-indexing-time.png\" width=\"800\"/>\n",
    "  <figcaption>Summary + Large Context RAG at Indexing Time</figcaption>\n",
    "</figure>\n",
    "\n",
    "At query time, we're still run the semantic search for the user's question against the _summaries_, then we'll use the stored chunk ID of the matching search result to retrieve the original large context chunk to use in the LLM prompt.\n",
    "<figure align=\"middle\">\n",
    "  <img src=\"./img/03c-summary-large-context-rag-query-time.png\" width=\"800\"/>\n",
    "  <figcaption>Summary + Large Context RAG at Query Time</figcaption>\n",
    "</figure>\n",
    "\n",
    "Once again, by performing the vector semantic search against the summaries, it increases the likelihood that a user's question will match the relevant piece of content. This is because the larger chunks reduces the chance that any information is taken out of context, and the summary process reduces any distracting noise that might be present in the original context. The summary preserves the primary _meaning_ of the document.\n",
    "\n",
    "Once we've found the ID of the matching content, we use it to retrieve the full large chunk text, and provide that to the LLM to use when answering the user's question.\n",
    "\n",
    "Importantly, this allows the LLM to have a rich set of information that very likely contains the answer to the user's question, and the LLM can answer to the same level of depth and nuance as represented in the original document.\n",
    "\n",
    "Now let's see how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking, embeddings, and Pinecone\n",
    "\n",
    "We'll re-use the `large_chunks`, `summary_documents`, and `summary_embeddings` from the previous section.\n",
    "\n",
    "However, we're going to modify what metadata we store in Pinecone. \n",
    "\n",
    "We want to be able to locate the original large chunk content, so we're going to save the index of the matching source document as the `source_id` in Pinecone. In your production app, you might store the S3 path, or the DynamoDB key, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 12}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_upload = [{\n",
    "    'id': f\"item-{i}\",\n",
    "    'values': summary_embeddings[i],\n",
    "    'metadata': {\n",
    "      'source': summary_doc.metadata['source'],\n",
    "      'source_id': f\"{i}\",\n",
    "    }\n",
    "  } for i, summary_doc in enumerate(summary_documents)]\n",
    "response = pinecone_index.upsert(vectors=to_upload, namespace=\"summary-plus-large-context-namespace\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we find a matching summary, we're going to use the `source_id` from the matching result to retrieve the full large chunk text, not just the summary, and this is what we'll send to the LLM.\n",
    "\n",
    "Let's define a function to do that, `retrieve_original_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_original_context(response):\n",
    "  context = \"\"\n",
    "  for match in response['matches']:\n",
    "    context += large_chunks[int(match['metadata']['source_id'])].page_content + \"\\n\\n\"\n",
    "  return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll re-run our vector search and LLM prompt against our original query, `\"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-----------------\n",
      "So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense.\n",
      "\n",
      "Step one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now.\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "\n",
      "-----------------\n",
      "\n",
      "Question: What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\n",
      "Short Answer:\n",
      "<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "LLM Response: - GPT-4 gave a nonsense answer when asked how long it would take to dry 30 clothes based on the information that it took 5 hours to dry 5 clothes.\n",
      "- GPT-4 gave a nonsense answer when asked how to measure 6 liters using a 12-liter jug and a 6-liter jug. It provided a complicated and unnecessary solution instead of simply using the 6-liter jug.\n",
      "- GPT-4 gave a nonsense answer when asked if bicycling over a bridge suspended over nails, screws, and broken glass would result in a flat tire. It failed to reason that the surface of the bridge doesn't directly touch the sharp objects.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"What are the examples where GPT-4 gave nonsense answers because it lacks common sense?\"\n",
    "query_emb = create_embeddings([query_str])[0]\n",
    "\n",
    "response = pinecone_index.query(vector=query_emb,\n",
    "                                namespace=\"summary-plus-large-context-namespace\",\n",
    "                                top_k=1,\n",
    "                                include_metadata=True)\n",
    "\n",
    "answer = run_llm_qa_prompt(context=retrieve_original_context(response), question=query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only is that the right answer, it's well reasoned! 🥳🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A detailed follow-up\n",
    "Let's see how it does on our more detailed follow up question: `\"Explain the example where GPT-4 failed to reason about drying clothes.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Prompt Start >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context:\n",
      "-----------------\n",
      "So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense.\n",
      "\n",
      "Step one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now.\n",
      "\n",
      "OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly.\n",
      "\n",
      "OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid.\n",
      "\n",
      "\n",
      "-----------------\n",
      "\n",
      "Question: Explain the example where GPT-4 failed to reason about drying clothes.\n",
      "Short Answer:\n",
      "<<< Prompt End <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "LLM Response: GPT-4 failed to reason about drying clothes because it incorrectly assumed that the time it takes to dry a certain number of clothes is directly proportional to the number of clothes. It concluded that if it took five hours to dry five clothes, then it would take 30 hours to dry 30 clothes, which is not a logical or accurate conclusion.\n"
     ]
    }
   ],
   "source": [
    "follow_up_query_str = \"Explain the example where GPT-4 failed to reason about drying clothes.\"\n",
    "follow_up_query_emb = create_embeddings([follow_up_query_str])[0]\n",
    "\n",
    "response = pinecone_index.query(vector=follow_up_query_emb,\n",
    "                                namespace=\"summary-plus-large-context-namespace\",\n",
    "                                top_k=1,\n",
    "                                include_metadata=True)\n",
    "\n",
    "answer = run_llm_qa_prompt(context=retrieve_original_context(response), question=follow_up_query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the LLM is able to explain the example to the same level of depth as the original text, because it's looking at the original text.\n",
    "\n",
    "So not only are we getting better quality output from our app, we're also requiring significantly less vector storage to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about cost\n",
    "\n",
    "Like with any database, the more you store in a vector database the more it's going to cost. So if we can _reduce_ the quantity of data we're putting in our vector database while _increasing_ the quality of our app's responses, that sounds like a double-win. In our case we reduced the quantity of vectors by 3 or 4x -- a significant cost savings over time.\n",
    "\n",
    "One thing to keep in mind with this summary strategy is that our costs at indexing time will increase due to the use of an LLM to create the summaries. Although this is likely a one-time cost if your dataset is static, it's still important to consider how it will impact the operation of your app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting your app in production\n",
    "\n",
    "There are a lot of factors to consider when you're putting a new app in production. Your choice of vector storage, indexing, and query strategies are just one piece of the puzzle. You also need to consider how your source data will change over time, and the data pipeline needed to keep it up to date.\n",
    "\n",
    "You need to consider your overall anticipated app usage and performance level needed, and balance these against requirements against the cost of building and operating your app to support these levels of usage, including managing your LLM cost.\n",
    "\n",
    "Ninetack can help you work through all these decisions, and help you get your app in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'd love to talk with you\n",
    "\n",
    "Ninetack is dedicated to helping our clients leverage the latest technologies to build innovative solutions for every industry.\n",
    "\n",
    "We'd love to talk with you about how you're planning to incorporate vector search in your next AI application. Connect with us today @ ninetack.io!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selectively run these as needed to clean up Pinecone. You can cleanup a specific namespace to start that section over, or you can remove the index completely when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.delete(delete_all=True, namespace=\"basic-rag-namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.delete(delete_all=True, namespace=\"summary-rag-namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.delete(delete_all=True, namespace=\"summary-plus-large-context-namespace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the index when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(\"ted-talk-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
